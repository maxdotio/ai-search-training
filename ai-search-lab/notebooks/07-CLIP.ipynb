{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a25145-4603-48e6-94a9-0b1539d3d990",
   "metadata": {},
   "source": [
    "# Lab 7 - Multimodal search with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eaaaf1-dccf-44fe-a681-9b49588cba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IP_Image, display\n",
    "from open_clip import tokenizer, create_model_and_transforms\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1fa3d-cbaa-4d94-bda5-4d287dc46809",
   "metadata": {},
   "source": [
    "## Get the image dataset (interiors of houses)\n",
    "\n",
    "- Source: https://www.kaggle.com/datasets/mikhailma/house-rooms-streets-image-dataset/data\n",
    "- Cached: https://max.io/house_data_png.zip (resized to 256x256 and converted to PNG)\n",
    "- License: CC-0 Public Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ec361-c278-4cb5-9c82-878c92668260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and extract the zip file\n",
    "def download_and_extract_zip(url, extract_to='.'):\n",
    "    print('Downloading and extracting',url)\n",
    "    response = requests.get(url)\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "# Download and extract the example images\n",
    "url = \"https://max.io/house_data_png.zip\"\n",
    "download_and_extract_zip(url)\n",
    "image_dir = 'house_data_png'\n",
    "image_paths = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir)]\n",
    "print('Extracted',len(image_paths),'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6185f03-fc2c-4aec-ba99-eb78d43f8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our model.\n",
    "model, transform, preprocess = create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "\n",
    "#Print the model architecture, note both the \"visual\" and \"transformer\" branches of the model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dedfd7-0d7a-4cd2-8521-3efd05aa57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infers images in batches.\n",
    "def get_image_embeddings(image_paths, batch_size=32):\n",
    "    embeddings = []\n",
    "\n",
    "    # Process images in batches\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Processing Images\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_images = [transform(Image.open(path)).unsqueeze(0) for path in batch_paths]\n",
    "\n",
    "        # Stack and process the batch\n",
    "        batch_images_tensor = torch.vstack(batch_images).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode_image(batch_images_tensor)\n",
    "\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    return torch.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86c3df-6a9b-4313-8fd6-202a141aeea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = get_image_embeddings(image_paths, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e59c9-ae46-41cb-aa96-133019d20502",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True) #Normalization is required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd0ce7-58ce-437b-ac85-b0b431e34154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('house_data_png.pkl', 'wb') as fd:\n",
    "    pickle.dump(image_embeddings.cpu().numpy(), fd, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36083c88-f25a-4178-a932-b2d5804a469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_embeddings),image_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b35845-38ce-49fe-a285-cdd6f81ef81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encodes the text to the same vector space as the images\n",
    "def embed_text(text):\n",
    "    tokens = tokenizer.tokenize([text])\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True) #Normalization is required!\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce99603-6d25-49ab-861c-4eed146e6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def display_images(image_paths,distances):\n",
    "    for idx,path in enumerate(image_paths):\n",
    "        display(IP_Image(filename=path))\n",
    "        print('ðŸ‘†',distances[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1a781-3a89-4e86-8232-ef8812b4a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will search and display nearest images given a text query\n",
    "nbrs = NearestNeighbors(n_neighbors=10, metric='cosine').fit(image_embeddings.cpu().numpy())\n",
    "def search(text):\n",
    "    text_embedding = embed_text(text)\n",
    "    distances, indices = nbrs.kneighbors(text_embedding.cpu().numpy())\n",
    "    nearest_images = [image_paths[i] for i in indices[0]]\n",
    "    display_images(nearest_images, distances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec92c08-f59d-4d70-b182-5eb717003619",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('large kitchen island colonial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35396557-103f-4012-8a98-63e6ac0375e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('white marble shower stall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c734cd0-4299-4852-b6cf-0e125c21d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('red ferrari')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0efd47-95bd-4ac8-9727-4695d6658f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('nuclear reactor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf8677-dc95-4ff0-8fd0-a5fb4ba6f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_image(index):\n",
    "    image_embedding = image_embeddings[index]\n",
    "    distances, indices = nbrs.kneighbors([image_embedding.cpu().numpy()])\n",
    "    nearest_images = [image_paths[i] for i in indices[0]]\n",
    "    display_images(nearest_images, distances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09b225-1f92-4457-bd2d-96106539e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_by_image(505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f78bf-d4f8-4407-93bd-5f6c4faaad94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
